---
title: "Evaluation of the Performance of Community Detection Algorithms on benchmark networks"
author: "Fabio Morea"
date: "2023-05-19"
abstract: "This notebook compares the performance of several community detection methods, and explores the potential improvements that can be obtained by the following techniques: pruning, consensus and pre-cut."
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE)
```

```{r load libraries, include=FALSE}
library(igraph)
library(tidyverse)
library(aricode) # NMI 
```

# 1- introduction

Network analysis is a powerful tool for understanding complex systems in
both the hard and social sciences. By representing data as a network, it
is possible to identify communities of similar members, or nodes, that
are connected to each other by various relationships. This allows to
gain insights into the structure of the system, as well as the behavior
of the nodes within it.

For example, in social networks, nodes can represent people, and the
relationships between them can represent various types of interactions,
such as friendships or collaborations. By analyzing the network,
researchers can identify clusters of people who are more likely to
interact with each other, or who share similar characteristics.

Our research focuses on the need to identify communities within
networks. This is an unsupervised task that presents several critical
issues, such as selecting the most appropriate algorithm and quantifying
the quality of the results.

The *definition of community* depends on the research context; our research is based on the definition of community as a subset of nodes
that are more strongly connected to each other than to the rest of the
network. 

A number of *Community Detection* (CD) *algorithms* can be used to
generate partitions of the network by exploiting its topology and
possibly the weight of ties. These methods are widely known and
implemented in programming languages such as R or Python, which enable
the results to be obtained quickly on large networks.

It is essential to recognize that the term 'community detection' can be
misleading. **Communities are not an inherent property of the network**;
rather, they are the outcome of an algorithm that is dependent on the
characteristics of the network, the parameters chosen, and often an
initial random seed. 

An algorithm always returns a results (i.e. a set of membership labels associated to vertices) but **it does not provide an explicit assessment of the quality of results**.

The **results vary depending on the chosen algorithm**. for example under some circumstances an algoritm may find no communities (i.e. membership vector is an array of '1'), chile another algorithm may find a number of different communities. 
Moreover, results can also **vary at each repetition of a CD algorithm**. Differences can be small if communities are well-defined, but may also vary significantly if communities are fuzzy. 

We assume that in real world problems CD is an unsupervised task (i.e. there are no "true labels" against which results can be tested. Therefore it is not reccommended to take the first result generated by a chosen algorithm as the "true" or "only" solution. 



Our research focuses on assessing the quality of results produced by CD algorithms with respect to the 'fuzzyness' of a network. Specifically, we aim to assign each node not only with a *membership label* but also with a *Robustness of Membership* (RM) score. This allows us to gain insights into the overall quality of the community detection process and to classify nodes according to their RM 
We propose that an unsupervides CD task should be carried out in 3 steps: 

1- explorative analysis of the fuzzyness of the whole network
2- apply an appropriate CD algorithm (if fuzzyness is low) or a combination of algorithms and methods (e.g. aggregation, pre-cut and consensus, as describet below)
3- express results as "membership" and "robustness of membership" associated with each node. 

## Contents of this notebook
This notebook addresses all the points mentioned above.

Secion 2 is focused on building and exploring the properties of a family of benchmark networks. we add some comments on the modularity of built-in communities, and show the parameter \mu governing the fuzzyness of the network is proportional to modularity of built-in communities $Q_bic$. 

Section 3 compares five CD algorithms methods on single trials and Section 4 on repeated trials. Results are assessed using $Q_bic$, Normalized Mutual Information ($NMI$) and  and Number of Communities ($NC$). results are stochastic for 3 of the algorithms considered. Overall, as network fuzzyness increases, results produced by each algorithm diverge significantly in distinct ways: overestimation, underestimation and collapse. 


Section 5 deals with potential improvements of algorithms that can be obtained by aggregation of small communities into a "meta-community" labelled '0'. 

Section 6 proposes a consensus algorithm based on independent repetitions of (any) given CD algorithm, followed by pairwise comparison of node membership. This results in robust communities (depending on a single threshold p_min) and a 'robustenss of membership' RM associated with each node.  

Section 7 deals potential improvements suitable for algorithms that aggregate results in large communities. The method is "pre-cut" a randomly selec

Finally, section 8 presents a method to assewithout built-in communities

Section 9 draws onclusions and briefly discusses future developments.

\newpage

# 2- benchmark networks and Indicators to assess performance of CD methods

## 2.1 benchmark networks
As the real-world problems are unsupervised, it is impossible to compare the performance of different methods. To study the performance of the method, we will use a family of networks with built-in communities that allow comparison with true labels.

LFR (Lancichinetti-Fortunato-Radicchi) benchmark graphs are synthetic
networks used to evaluate the performance of network analysis
algorithms. They are generated using a stochastic block model and are
designed to have realistic community structure, degree distributions,
and other properties of real-world networks.

They are used to test the accuracy of algorithms for network analysis
tasks such as community detection, link prediction, and node
classification.

Benchmark graphs for testing community detection algorithms, Andrea
Lancichinetti, Santo Fortunato, and Filippo Radicchi, Phys. Rev. E 78,

046110 2008 [LFR_benchmark_graph --- NetworkX 3.1
documentation](https://networkx.org/documentation/stable/reference/generated/networkx.generators.community.LFR_benchmark_graph.html)

The following code chunk defines a function to load a benchmark network
(previously generated).

```{r  echo=TRUE}
load_benchmark_network <- function(mui, path = path, verbose = FALSE) {
    #upload graph
    filename = paste0(path, mui, ".gml")
    g <- read_graph(filename, format = "gml")
    # extract giant component
    #components <- igraph::clusters(g, mode = "weak")
    #g <-induced_subgraph(g, V(g)[components$membership == which.max(components$csize)])
    # set names and weight (ww <- 1.0)
    V(g)$core <- coreness(g)
    V(g)$str <- strength(g)
    V(g)$name <- paste0("V" , V(g)$label)
    E(g)$ww <- 1.0
    # print
    if (verbose == TRUE) {
        print(paste0("Loaded benchmark network ", path, mui, ".gml"))
        print(paste("Giant component size :", length(V(g)$community)))
        print(paste("Built-in communities: ", max(V(g)$community)))
        mod <- round( modularity(g, array(V(g)$community+1) ), digits = 4)
        print(paste("Modularity of built-in communities: ", mod))
    } 
    return(g)
}
```

To illustrate the structure of a reference netowotk, we load a network
with well-defined communities ($mu = 0.20$) and show a coreness-strength
scatterplot.

K-coreness is a measure of the centrality of a node within a network. It
is defined as the largest value of k such that the node belongs to a
subgraph of the network that is composed of nodes with at least degree
k. A node with a high k-coreness is considered to be more central and
influential within the network.

Strength is a measure of the total weight of the connections a node has
to other nodes in the network.

```{r scatter Kcore-Sterngth benchmark, echo=TRUE, message=FALSE, warning=FALSE}
path <- "./LFR_graphs/LFR/LFR_benchmark_"
g <- load_benchmark_network(mui = 20, path = path, verbose = TRUE)
sc_data <- data.frame(c = V(g)$core, s = V(g)$str)
sc_data %>% ggplot(aes(x = c, y = s))+
    geom_point(color = "red", size = 3, alpha = .2)+
    labs(title = "coreness and strength of a benchmark network", x = "k-coreness", y = "strength") +     theme_classic() 
```

```{r scatter nc degree, echo=TRUE, message=FALSE, warning=FALSE}
dg <-  degree(g)
ncd_data <- data.frame(d = dg, nc = as.factor(V(g)$community))
ncd_data %>% ggplot(aes(x = d, y = nc, color = nc))+
  geom_vline(xintercept = mean(dg),  color = 'black', linewidth = 1, alpha = 1)+
  geom_point( size = 3, alpha = .2)+
  labs(title = "built-in communities", x = "degree", y = "community ID") +     theme_classic() 
```

The family of benchmark networks can be characterized by the modularity
of its built-in communities.

```{r}
mod_benchmarks <- data.frame(method = character(), mu = numeric(), modularity = numeric())
method <- "built-in"
for (mu in seq(10, 70, 5)) {
    g <- load_benchmark_network(mui = mu, path = path)
    mod <- modularity(g, make_clusters(g,V(g)$community+1 )$membership )
    mod_benchmarks <- rbind(mod_benchmarks, data.frame(method,mu, mod))
}
mod_benchmarks %>% 
  ggplot(aes(x = mu, y = mod)) +
  geom_line(linewidth = 2, color = 'gray')+
  geom_point()+
  ylim(0,1) + theme_classic()
```

Methods based on modulartiy maximization are less effective with high
values of \mu.

\newpage

## 2.2 fuzzyness on communities within a network
In broad terms, "fuzziness" of communities within a network is a measure of how well-defined they are. This concept is closely related to the definition of "community" and can be assessed using "modularity" metrics.
A good proxy for fuzzyness can be derived from Modularity $Q$ of a given set of communities. If the network has built-in communities, $Q$ can be calculated directly. Otherwise, we are facing an unsupervised machine learning task: we assume that we have no prior knowledge on the existence of communities, hence NC may range from 0 (no communities, all nodes have the same membership label) to Nv (each node is an independent community). The fuzziness of a netwhork can be assumed as the maximum value of $Q$ obtained by N repetitions of different modularity maximization algorithms. T 


## 2.2 indicators

Modularity $Q$ and Max(Q)that is a proxy of network fuzzyness

NMI 
NC number of communities NMI can be useful to measure the agreement
of two independent label assignments on the same dataset when the real
ground truth is not known.

\newpage

# 3- community definitions and community detection methods

## 3.1 definition of community 

The definition of 'community' in network analysis depends on the context
and the researcher's goals. for example:

1.  a community is a group of nodes that are more densely connected to
    each other than to the rest of the network.

2.  a community is group of nodes that share certain characteristics,
    such as having similar properties

3.  a community is a group of nodes that are more likely to interact
    with each other than with other nodes in the network.


We assume the first definition and test the following algorithms: 

-   FG Fast Greedy
-   IM infomap
-   LP Label Propagation
-   ML Multilevel (aka Louvain)
-   WT walktrap

    
## ideal and real CD algorithm
Ideally, result obtained by a CD algorithm should be similar to that
obtained by other methods based on the same definition of community. This happens in networks with well-defined communities; as fuzzyness increases, results diverge.

Moreover, an ideal CD  algorithm should produce the same result at every run (it should not be stochastic, i.e. it should not depend on random initialization). 
However, many algorithms adopt a stochastic approach that increases speed. We argue that, when using a stochastic algorithm the uncertainty associated with its results should be carefully evaluated on the whole network and on the individual node. 
Specifically, we argue that, before selecting a stochastic CD algorithm for a new network, an *exploratory analysis of network fuzzyness* should be performed to asses the overall fuzzyness of the network (i.e. are there any easily identifiable communities within the network?). If fuzzyness is low, an appropriate CD method can be selected, and results can be confidently interpreted on a single run. In all other cases a more complex approach is required. The researcher must be aware that different methods will generate underestimates or overestimates of the number of communities, associated with great variability of results. In this context CD is still possible by using the following additional steps:
- *aggregation* of small communities (for CD algorithms that produce highly fragmented results)
- *consensus* among independent trials of the same method (that allows to estimate RM robustness of membership)
- *pre-cut* of network (to imporve performance of CD algorithms that produce few large communities)


```{r function_find_communities}
find_communities <- function(g, method, verbose = FALSE) {
    if (method == "LV"){
        comms <- cluster_louvain(g, resolution = 1.0)
    } else if (method == "FG"){
        comms <- fastgreedy.community(g)
    } else if (method == "IM"){
        comms <- infomap.community(g)
    } else if (method == "LP"){
        comms <- label.propagation.community(g)
    } else if (method == "ML"){
        comms <- multilevel.community(g)
    } else if (method == "WT"){
        comms <- walktrap.community(g)
    } else {
        print("No valid method")
        stop
    }
    comms$algorithm = method
    if (verbose == TRUE) {print(paste("Community detection with ", method, "completed."))}
    return(comms)
}
```

Each method will be explored separately, in a single trial on a LFR
benchmark network, highlighting the main results with the followin
gfunction

```{r function_analyse_communities}
analyse_communities <- function(communities, mui, verbose = FALSE){
    method <- communities$algorithm
    c_membership <- communities$membership
    mod <- round(modularity (g,  c_membership+1), digits = 4)
    # need to use c_membership + 1 to handle community label 0
    nc <- length(table(c_membership))
    # NMI against "Built In Communities"
    nmi = round(aricode::NMI( as.factor(V(g)$community),as.factor(c_membership)), 3)
    if(verbose == TRUE){
        print(paste("Communities found: ",nc))
        print(paste("Modularity: ", mod))
        print(paste("Normalized Mutual Information between C.D. method and built-in communities:", nmi))
    } 
    return(data.frame(mui, method , mod, nc, nmi))
}
```

all trials use the same network

```{r}
mui = 30
g <- load_benchmark_network(mui = mui, path = path, verbose = TRUE)
```

\newpage

## 3.1 Method FG Fast Greedy

This algorithm was proposed by Clauset et al.12. It is a greedy
community analysis algorithm that optimises the modularity score. This
method starts with a totally non-clustered initial assignment, where
each node forms a singleton community, and then computes the expected
improvement of modularity for each pair of communities, chooses a
community pair that gives the maximum improvement of modularity and
merges them into a new community. The above procedure is repeated until
no community pairs merge leads to an increase in modularity. For sparse,
hierarchical, networks the algorithm runs in $O$(N log2(N)).


```{r}
g <- load_benchmark_network(mui = mui, path = path)
tmp_comms <- find_communities(g, method = "FG", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

## 3.2 Method IM infomap

This algorithm was proposed by Rosvall et al.35,36. It figures out
communities by employing random walks to analyse the information flow
through a network17. This algorithm starts with encoding the network
into modules in a way that maximises the amount of information about the
original network. Then it sends the signal to a decoder through a
channel with limited capacity. The decoder tries to decode the message
and to construct a set of possible candidates for the original graph.
The smaller the number of candidates, the more information about the
original network has been transferred. This algorithm runs in $O$(E).

```{r}
tmp_comms <- find_communities(g, method = "IM", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

## 3.3 method LP Label Propagation

This algorithm was introduced by Raghavan et al.38. It assumes that each
node in the network is assigned to the same community as the majority of
its neighbours. This algorithm starts with initialising a distinct label
(community) for each node in the network. Then, the nodes in the network
are listed in a random sequential order. Afterwards, through the
sequence, each node takes the label of the majority of its neighbours.
The above step will stop once each node has the same label as the
majority of its neighbours. The computational complexity of label
propagation algorithm is $O$(E).

```{r}
g <- load_benchmark_network(mui = mui, path = path)
tmp_comms <- find_communities(g, method = "IM", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

## 3.4 method ML Multilevel (Louvain)

This algorithm was introduced by Blondel et al.. It is a different
greedy approach for optimising the modularity with respect to the
Fastgreedy method. This method first assigns a different community to
each node of the network, then a node is moved to the community of one
of its neighbours with which it achieves the highest positive
contribution to modularity. The above step is repeated for all nodes
until no further improvement can be achieved. Then each community is
considered as a single node on its own and the second step is repeated
until there is only a single node left or when the modularity can't be
increased in a single step. The computational complexity of the
Multilevel algorithm is $O$(N log N). Blondel, V. D., Guillaume, J.-L.,
Lambiotte, R. & Lefebvre, E. Fast unfolding of communities in large
networks. Journal of Statistical Mechanics: Theory and Experiment 2008,
P10008 (2008).

```{r}
g <- load_benchmark_network(mui = mui, path = path)
tmp_comms <- find_communities(g, method = "ML", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

## 3.5 method WT walktrap

This algorithm was proposed by Pon & Latapy. It is a hierarchical
clustering algorithm. The basic idea of this method is that short
distance random walks tend to stay in the same community. Starting from
a totally non-clustered partition, the distances between all adjacent
nodes are computed. Then, two adjacent communities are chosen, they are
merged into a new one and the distances between communities are updated.
This step is repeated (N-1) times, thus the computational complexity of
this algorithm is $O$(E N2). For sparse networks the computational
complexity is $O$(N2 log(N)

```{r}
g <- load_benchmark_network(mui = mui, path = path)
tmp_comms <- find_communities(g, method = "WT", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

\newpage

# 4- performance of 6 CD methods on LFR benchmark networks (repeated trials)

Exploring the variations of results: we generate 200 trials for a range
of MU and save results in a csv file.

```{r MAIN SIMULATION, eval=FALSE, include=FALSE}
find_communities_N_times <- function(n_trials,
                                     methods,
                                     mu_values,
                                     filename_summary,
                                     filename_membership) {
  results_n_trials <- data.frame()
  membership_matrix <- c()
  for (mu in mu_values) {
    g <- load_benchmark_network(mui = mu, path = path)
    for (i in 1:n_trials) {
      for (met in methods) {
        tmp_comms <- find_communities(g, method = met)
        results_single <- analyse_communities(tmp_comms, mui)
        results_n_trials = rbind(results_n_trials, results_single)
        membership_matrix <- cbind(membership_matrix, tmp_comms$membership)
      }
    }
  }
  membership_n_trials <- data.frame(membership_matrix)
  membership_n_trials %>% write_csv(filename_membership)
  results_n_trials %>% write_csv(filename_summary)
  return(1)
}

```
 

```{r}
find_communities_N_times(
  n_trials = 10 ,
  methods  = c("FG", "IM", "LP", "ML", "WT"),
  mu_values = seq(10, 70, 5),
  filename_summary = 'n_trials_summary_results.csv',
  filename_membership = 'n_trials_membership.csv'
) 
```

```{r read csv repeated results, message=FALSE, warning=FALSE}
summary_results<- read_csv('n_trials_summary_results.csv')  
#membership_matrix <- as.matrix(read_csv('n_trials_membership.csv'))
```

## 4.1 performance of different methods over repeated trials: modularity

```{r message=FALSE, warning=FALSE, include=FALSE}
mod_to_plot <- summary_results %>% group_by(method, mui) %>% 
    summarise(m = mean(mod), sd = sd(mod) )%>%
  mutate(mu = mui) %>% mutate(mod = m) %>%
  select(method, mu, mod, sd) %>% 
  rbind(mod_benchmarks) %>% 
  mutate(nm = mod/mod_benchmarks$mod) 
```

```{r}
mod_to_plot %>%
  ggplot(aes(x = mu, y = mod)) +
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
  
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
  geom_line(data = subset(mod_to_plot, method != 'built-in'), aes(color = method)) +
  geom_line(data = subset(mod_to_plot, method == 'built-in'), size = 2, color = 'gray') +
  geom_point(data = subset(mod_to_plot, method != 'built-in'), aes(color = method)) +
  geom_point(data = subset(mod_to_plot, method == 'built-in'), size = 1, color = 'black') +
  
     geom_hline(yintercept = 0.5)+
    labs(title = "performance of different methods (modularity)", y = "modularity", x = "mu") +
    theme_light()   

```

Note that when modularity \< 0.5, the definition of community is not
consistent.
WHen LP returns modularity > 0.5 it is very accurate. returns modularity < 0.5, the algorithm fails. This is a very relevant insight
try modularity / modularityBI

```{r}
mod_to_plot %>% filter(method != 'built-in') %>%
  ggplot(aes(x = mu, y = nm)) +
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 2 ), fill = "green", alpha = 0.005)+
  geom_rect( aes(NULL, NULL, xmin = 35, xmax = 50, ymin = 0, ymax = 2 ), fill = "lightblue", alpha = 0.01)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 2 ), fill = "yellow", alpha = 0.005)+

  geom_line( aes(color = method)) +
  geom_point( aes(color = method)) + 
    labs(title = "different performance of CD methods as a function of network fuzzynes", y = "modularity method / modularity built-in", x = "mu") +
    theme_minimal()   

```

## 4.2 performance of different methods over repeated trials: NMI

NMI can be calculated against the built-in communities (NMI-BI) or
against all other methods (NMI_OM).

```{r}
summary_results %>% 
    filter(method %in% c( "FG", "IM", "LP", "ML", "WT")) %>%
  ggplot(aes(x = mui, y = nmi)) +
    geom_point(aes(color = method)) +
    labs(title = "Performance of different methods over N trials", y = "NMI_BIC", x = "mu") +
    theme_light()  + 
    facet_wrap(method ~ .) #,  scales = "free_x"
```

```{r}
summary_results %>% 
  filter(mui %in% c(40)) %>%
  filter(method %in% c( "FG", "IM", "LP", "ML", "WT")) %>%
    ggplot(aes(x = nmi)) +
    geom_histogram(color = "black", fill = "red", bins = 100) +
    labs(title = "Performance of different methods over N trials mu = 40", x = "Normalized Mutual Information", y = "count") +
    geom_vline(xintercept = 0.5, color = 'red')+
    theme_classic() + 
    facet_wrap(method ~ .)
```

\newpage

The following Figure compares the results of each methods with the built
in labels, using NMI.

```{r}
summary_results %>% group_by(method, mui) %>% 
    summarise(mean_nmi = mean(nmi), sd_nmi = sd(nmi) ) %>%
    ggplot(aes(x = mui, y = mean_nmi)) +
    geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
    geom_rect( aes(NULL, NULL, xmin = 35, xmax = 50, ymin = 0, ymax = 1 ), fill = "lightblue", alpha = 0.01)+
    geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
    geom_line(aes(color = method)) +
    geom_point(aes(color = method)) +
    geom_linerange(aes(ymin =mean_nmi-sd_nmi, ymax = mean_nmi + sd_nmi, color = method), linewidth = 2, alpha = 0.5)+
    geom_hline(yintercept = 0.5)+
    labs(title = "performance of different methods ", y = "NMI_BIC", x = "mu") +
    theme_light()  
```

The following Figure compares the results of each methods with the built
in labels, using NMI.

```{r}
summary_results %>% group_by(method, mui) %>% 
    summarise(mean_nmi = mean(nmi), sd_nmi = sd(nmi), 
              mean_mod = mean(mod), sd_mod = sd(mod) ) %>%
    ggplot(aes(x = mean_mod, y = mean_nmi)) +
    geom_rect( aes(NULL, NULL, xmin = 0.6 , xmax = 1.0, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
    geom_rect( aes(NULL, NULL, xmin = 0.35 , xmax = 0.6, ymin = 0, ymax = 1 ), fill = "lightblue", alpha = 0.01)+
    geom_rect( aes(NULL, NULL, xmin = 0.0 , xmax = 0.35, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+

      geom_line(aes(color = method)) +
    geom_point(aes(color = method)) +
    #geom_linerange(aes(ymin = mean_nmi - sd_nmi, ymax = mean_nmi + sd_nmi, color = method), linewidth = 2, alpha = 0.5)+
    #geom_linerange(aes(xmin = mean_mod - sd_mod, xmax = mean_mod + sd_mod, color = method), linewidth = 2, alpha = 0.5)+
      geom_hline(yintercept = 0.5)+
    labs(title = "performance of different methods ", y = "NMI_BIC", x = "modularity") +
    theme_minimal()  
```

```{r}
 data_points <- summary_results
data_lines <- summary_results %>% group_by(method, mui) %>% 
    summarise(mean_nmi = mean(nmi), sd_nmi = sd(nmi), 
              mean_mod = mean(mod), sd_mod = sd(mod) ) 
ggplot() +
    geom_line(data = data_lines, aes(x = mean_mod, y = mean_nmi, color = method)) +
    geom_linerange(data = data_lines, aes(x = mean_mod, y = mean_nmi,ymin = mean_nmi - sd_nmi, ymax = mean_nmi + sd_nmi, color = method), linewidth = 2, alpha = 0.5)+
    geom_linerange(data = data_lines, aes(x = mean_mod, y = mean_nmi,xmin = mean_mod - sd_mod, xmax = mean_mod + sd_mod, color = method), linewidth = 2, alpha = 0.5)+
      geom_point(data = data_points, aes(x = mod, y = nmi, color = method)) +

      geom_hline(yintercept = 0.5)+
    labs(title = "performance of different methods ", y = "NMI_BIC", x = "modularity") +
    theme_light()  + facet_wrap(method ~ .)
```

\newpage

## community detection to assess "fuzzyness"

When dealing with an unsupervised community detection task, it is
important to remember that the term "detection" is misleading.
Communities are not an intrinsic property of the network, but rather
arise as the result of an algorithm. The results depend on network
properties, the chosen algorithm, its parameters, and its intrinsic
stochasticity.

Additionally, networks may generate communities with different levels of
fuzziness. Low fuzziness refers to well-defined communities in which
members have strong connections with each other and weak connections to
members of other communities. High fuzziness, on the other hand, refers
to communities in which members have weaker connections with each other
and stronger connections to members of other communities. In between
these two extremes, there is a transition zone, where the strongly
fuzziness depends on the method used to measure it.

Low fuzzyness: modularity is HIGH \> 0.5 is always high regardless of
the method. LP and FG are sensitive to fuzzyness: when their results
drop below 0.5, we are in the transition TRY NMI without true labels

```{r}
summary_results %>% 
  filter(mui %in% c(10,20,30)) %>% 
  ggplot(aes(x = mod, y = nmi, color = method)) +
  geom_point( size = 4, alpha = 0.2) +
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 0.5, color = "red")+
    labs(title = "performance of different methods LOW FUZZYNESS", y = "NMI_BIC", x = "modularity") +
  theme_light()  + facet_wrap(mui ~ .)
```

```{r}
summary_results %>% 
  filter(mui %in% c(35,40)) %>% 
  ggplot(aes(x = mod, y = nmi, color = method)) +
  geom_point( size = 4, alpha = 0.5) +
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 0.5, color = "red")+
  labs(title = "performance of different methods TRANSITION ", y = "NMI_BIC", x = "modularity") +
  theme_light()  + facet_wrap(mui ~ .)
```

when most methods generate low NMI, we

```{r}
summary_results %>% 
  filter(mui %in% c(50,60)) %>% 
  ggplot(aes(x = mod, y = nmi, color = method)) +
  geom_point(size = 4, alpha = 0.2 ) +
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 0.5, color = "red")+
  labs(title = "performance of different methods HIGH FUZZYNESS ", y = "NMI_BIC", x = "modularity") +
  theme_light()  + facet_wrap(mui ~ .)
```

\newpage

```{r}
summary_results %>% 
  filter(mui %in% c(20,40,60)) %>% 
  group_by(method, mui) %>% 
    summarise(mean_nmi = mean(nmi), sd_nmi = sd(nmi), 
              mean_mod = mean(mod), sd_mod = sd(mod) ) %>%
  mutate( fuzzyness = case_when(
    mui == 20 ~ '  low fuzzyness',
    mui == 40 ~ ' transition',
    mui == 60 ~ 'high fuzzyness'))%>%
  ggplot(aes(x = mean_mod, y = mean_nmi, shape = as.factor(method), color = fuzzyness)) +
  geom_point(size = 3, alpha = 1 ) +
  geom_linerange(aes(x = mean_mod, y = mean_nmi,ymin = mean_nmi - sd_nmi, ymax = mean_nmi + sd_nmi), 
                 linewidth = 2, alpha = 0.5)+
geom_linerange(aes(x = mean_mod, y = mean_nmi,xmin = mean_mod - sd_mod, xmax = mean_mod + sd_mod), linewidth = 2, alpha = 0.5)+
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 0.5, color = "red")+
  xlim(0,1)+#ylim(0,1)+
  labs(title = "Fuzzyness of communities", x = "modularity") +
  theme_light()  #+ facet_wrap(fuzzyness ~ .)
```

-   Low fuzzyness: all methods converge to the same result. Modularity
    is above 0.5
-   High fuzzyness: LP collapses to a single community, all other
    methods generate m \< 0.5
-   Transition:Other methods diverge: IM, ML and WT are above; FG drops
    below 0.5; LP generates high variance, including collapse.

if Low fuzzyness: use LP results if Transition: use IM, ML or WT, and
aggregate small communities if High fuzzyness: use IM or WT, precut,
consensus and aggregate small communities \newpage

## 4.3 performance of different methods over repeated trials: Number of Communities

```{r}
summary_results %>% 
    filter(method %in% c( "FG", "ML")) %>%
  ggplot(aes(x = mui, y = nc)) +
    geom_point(aes(color = method)) +
    labs(title = "Performance of different methods over N trials", y = "Number of Communitiess", x = "mu") +
  geom_hline(yintercept = 37)+
    theme_light() + 
    facet_wrap(method ~ . )
```

\``{r} summary_results %>%      filter(method %in% c( "LP")) %>%   ggplot(aes(x = mui, y = nc)) +     geom_point(aes(color = method)) +     labs(title = "Performance of different methods over N trials", y = "Number of Communitiess", x = "mu") +   geom_hline(yintercept = 37)+     theme_light() +      facet_wrap(method ~ . )`

```{r}
summary_results %>% 
    filter(method %in% c( "IM",  "WT")) %>%
  ggplot(aes(x = mui, y = nc)) +
    geom_point(aes(color = method)) +
    labs(title = "Performance of different methods over N trials", y = "Number of Communities", x = "mu") +
  geom_hline(yintercept = 37)+
    theme_light() + 
    facet_wrap(method ~ . , scales = 'free_y')
```

```{r}

summary_results %>% 
    group_by(method, mui) %>% 
    summarise(mean_nc = mean(nc), sd_nc = sd(nc)) %>%
    ggplot(aes(x = mui, y = mean_nc)) +
    geom_point(aes(color = method)) +
    geom_line(aes(color = method)) +
    geom_linerange(aes(ymin =mean_nc-sd_nc, ymax = mean_nc + sd_nc, color = method), linewidth = 2, alpha = 0.5)+
    geom_hline(yintercept = 37, color = 'red')+

    labs(title = "performance of different methods (Number of Communities)", y = "number of communities", x = "mu") +
    theme_light()    
```

\newpage

```{r}
summary_results %>% 
  filter(mui %in% c(20,40,60)) %>% 
  group_by(method, mui) %>% 
    summarise(mean_nc = mean(nc)/37, sd_nc = sd(nc)/37, 
              mean_mod = mean(mod)  ) %>%
  mutate( fuzzyness = case_when(
    mui == 20 ~ '  low fuzzyness',
    mui == 40 ~ ' transition',
    mui == 60 ~ 'high fuzzyness'))%>%
  ggplot(aes(x = mean_mod, y = mean_nc, shape = as.factor(method), color = fuzzyness)) +
  geom_point(size = 3, alpha = 1 ) +
  geom_linerange(aes(x = mean_mod, y = mean_nc,ymin = mean_nc - sd_nc, ymax = mean_nc + sd_nc), 
                 linewidth = 2, alpha = 0.5)+
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 1, color = "red")+
  xlim(0,1)+#ylim(0,1)+
  labs(title = "Fuzzyness of communities", x = "modularity") +
  theme_light() # + facet_wrap(fuzzyness ~ .)
```

How to improve results if fuzzyness low: use LP or WT if fuzzyness high:
use WT, IM with consensus collapse. or use ML, FG with precut consensus
and collapse if transition: use WT, IM with collapse \newpage

## 4.4 overal performance NMI NC

Performance measured as NMI and NC (improvement: normalize NC/37)

```{r}
summary_results %>% filter(mui %in%c(20,40,60)) %>%
    ggplot(aes(x = nmi, y = nc/37)) +
    geom_point(aes(color = method))+
    labs(title = "performance of different methods (NMI VS Number of Communities normalized)", 
         y = "number of communities", x = "NMI") +
    geom_hline(yintercept = 1.0, color = 'red')+
    geom_vline(xintercept = 1.0, color = 'red')+
    theme_light()  + facet_wrap(mui ~ .)
```

\newpage

## comparing NMI with other methods (without Built-in communities)

```{r compare all NMI, message=FALSE, warning=FALSE}
membership_matrix <- as.matrix(read_csv('n_trials_membership.csv'))
nmis <- data.frame()
for (mui in seq(10, 70, 5)) {
  membership_matrix_mu <- membership_matrix[, summary_results$mui == mui]
  methods <- summary_results$method[summary_results$mui == mui]
  mts <- c("FG", "IM", "LP", "ML", "WT")
  for (met1 in 1:length(mts)) {
    for (met2 in met1:length(mts)) {
      if (met1 != met2) {
        mm1 <- mts[met1]
        mm2 <- mts[met2]
        mb1 <- membership_matrix_mu[, methods == mm1]
        mb2 <- membership_matrix_mu[, methods == mm2]
        #print(paste(mui,"calculating NMI between methods", met1, met2))
        for (i in 1:ncol(mb1)) {
          for (j in i:ncol(mb2)) {
            if (i != j) {
              nmi_m1_m2 <- aricode::NMI(mb1[, i], mb2[, j])
              nmis <-rbind(nmis,data.frame(mui, mm1,mm2, met1, met2, nmi_m1_m2))
            }
          }
        }
      }
    }
  }
}
nmis %>% write_csv('NMIs.csv')
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
nmis <- read_csv('NMIs.csv') %>% select(mm1, mm2, nmi_m1_m2, mui)
NMI <- nmis %>% 
  group_by(mui, mm1, mm2)%>%
  summarize(nmi = mean(nmi_m1_m2), sd_nmi =sd(nmi_m1_m2))%>%
  mutate(methods = str_c(mm1, "-", mm2))

NMI%>%
  ggplot()+
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.002)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.002)+
  geom_line(aes(x = mui, y = nmi, color = methods)) + 
  #geom_linerange(aes(x = mui, y = nmi ,ymin = nmi - sd_nmi, ymax = nmi + sd_nmi), linewidth = 2, alpha = 0.5)+
  theme_light()+ggtitle("NMI - Pairwise comparison ")


```

...
...
...

```{r echo=TRUE, message=FALSE, warning=FALSE}
nmis <- read_csv('NMIs.csv')
NMI <- nmis %>% 
  group_by(mui, mm1, mm2)%>%
  summarize(nmi = mean(nmi_m1_m2), sd_nmi =sd(nmi_m1_m2))%>%
  mutate(methods = str_c(mm1, "-", mm2))

NMI%>% filter(methods %in% c( "IM-LP", "LP-ML", "LP-WT"))%>%
  ggplot()+
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
  geom_line( aes(x = mui, y = nmi, color = methods)) + 
  #geom_linerange(aes(x = mui, y = nmi ,ymin = nmi - sd_nmi, ymax = nmi + sd_nmi), linewidth = 2, alpha = 0.5)+
  theme_light()+ggtitle("NMI - Pairwise comparison with LP")


```


```{r}
NMI%>% filter(methods %in% c("IM-WT", "FG-ML", "IM-FG"))%>%
  ggplot()+
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
  geom_line( aes(x = mui, y = nmi, color = methods)) + 
  #geom_linerange(aes(x = mui, y = nmi ,ymin = nmi - sd_nmi, ymax = nmi + sd_nmi), linewidth = 2, alpha = 0.5)+
  theme_light()+ggtitle("NMI - Pairwise comparison - extremes ")
```



```{r}
nmis %>% filter(mui %in% c(10,30, 50))%>%
  ggplot()+
  geom_point(aes(x=mm1, y = mm2, size = nmi_m1_m2^2, color = nmi_m1_m2))  + 
  scale_colour_gradientn(colours=rainbow(2))+
  facet_wrap(mui ~ .) + theme_light() + 
  ggtitle("Normalized Mutual Information between results obtained by different CD methods")
```

```{r fig.height=3, fig.width=6}
edgelist <-nmis %>% mutate(weight = nmi_m1_m2) %>% filter(mui == 10) %>% select(mm1, mm2, weight) 
ggg <- graph_from_data_frame(edgelist)
ggg <- as.undirected(ggg,mode = "collapse", edge.attr.comb = "sum")
plot(ggg, edge.width=E(ggg)$weight*0.5, vertex.size =50, vertex.color = "white")
```

```{r fig.height=3, fig.width=6}
edgelist <-nmis %>% mutate(weight = nmi_m1_m2) %>% filter(mui == 30) %>% select(mm1, mm2, weight) 
ggg <- graph_from_data_frame(edgelist)
ggg <- as.undirected(ggg,mode = "collapse", edge.attr.comb = "sum")
plot(ggg, edge.width=E(ggg)$weight*0.5, vertex.size =50, vertex.color = "white")
```
```{r fig.height=3, fig.width=6}
edgelist <-nmis %>% mutate(weight = nmi_m1_m2) %>% filter(mui == 50) %>% select(mm1, mm2, weight) 
ggg <- graph_from_data_frame(edgelist)
ggg <- as.undirected(ggg,mode = "collapse", edge.attr.comb = "sum")
plot(ggg, edge.width=E(ggg)$weight*0.5, vertex.size =30, vertex.color = "white")
```

\newpage

# 5- potential improvements by aggregation of small communities

re-assigning small communities to "community 999" can improve NMI and NC
of course only for the methods that overestimate NC

```{r pruning definition}
aggregate_small_communities <- function(comms, min_vids, verbose = FALSE) {
    comms_membership <- comms$membership 
    cs <- table(comms_membership)

    for (i in names(cs)) {
        if (cs[i] < min_vids) {
            comms_membership[comms_membership == i] <- 0
        }
    }
    comms$membership <- comms_membership
    if (verbose == TRUE) { 
        print(paste("Aggregatio of communities below ", min_vids, "completed."))}
    return(comms)
}

```

Example of aggregation

```{r pruning example1}
mui = 30
method = "IM"
g <- load_benchmark_network(mui = mui, path = path, verbose = TRUE)
comms_sample <- find_communities(g, method = method, verbose = TRUE)
results <- analyse_communities(comms_sample, mui, verbose = TRUE)

comms_aggregated <- aggregate_small_communities(comms_sample, min_vids = 10, verbose = TRUE)
results_aggregated <- analyse_communities(comms_aggregated, mui, verbose = TRUE)
```

The number of communities is closer to the built-in value (37), while
there is no relevant change in NMI.

The following shows how pruning impacts NC, usimg method IM and
$\mu = 0.30$

```{r pruning example3}
mui = 30
method = "IM"
g <- load_benchmark_network(mui = mui, path = path, verbose = TRUE)
comms_sample <- find_communities(g, method = method, verbose = TRUE)
results <- analyse_communities(comms_sample, mui, verbose = TRUE)
for (min_vids in 1:12){
    comms_aggregated <- aggregate_small_communities(comms_sample, min_vids = min_vids, verbose = FALSE)
    results_aggregated <- analyse_communities(comms_aggregated, mui , verbose = FALSE)
    print(paste0("min_vids = ", min_vids, "  NMI = ", results_aggregated$nmi,"  NC = ",results_aggregated$nc ))
}

```

\newpage

# 6- potential improvements by consensus

Consensus helps improving performance (applicable to methods that
underestimate NV, as it generates a number of small communities, AND
have some intrinsic variability of results)

```{r}
normalized_co_occurrence <- function(all_clusters) {
  n_trials <- ncol(all_clusters)
  x <-matrix(0,
           nrow = nrow(all_clusters),
           ncol = nrow(all_clusters))
  colnames(x) <- V(g)$name
  rownames(x) <- V(g)$name
  for (i in (1:n_trials)) {
    nclusters <- max(all_clusters[, i])
    for (k in 1:nclusters) {
      samecluster <- (which(all_clusters[, i] == k))
      nc <- length(samecluster)
      for (t in 1:nc) {
        for (j in 1:nc) {
          x[samecluster[j], samecluster[t]] <-
            x[samecluster[j], samecluster[t]] + 1
        }
      }
    }
  }
  x <- x / ncol(all_clusters)
  diag(x) <- 0
  return (x)
}
```

\newpage

```{r}
consensus <- function(membership_matrix, min_p = 0.5) {
  nodes_to_process <- nrow(membership_matrix)
  nco <- normalized_co_occurrence(membership_matrix)
  nodes_to_process <- nrow(nco)
  
  cons_results <- data.frame(name = V(g)$name)
  cons_results$membership = rep(0, nrow(cons_results))
  
  max_by_row <- apply(nco, 1, max)
  cons_results$rob = max_by_row
  
  # removing nodes that have p<min_p
  
  community_label <- 0
  nodes_to_process <- ncol(nco)
  
  while ( nodes_to_process > 1 )  {
    
    # remove rows and cols that are below threshold
    max_by_row <- apply(nco, 1, max)
    above_min_p <- (max_by_row >= min_p)
    nodes_to_process <- sum(above_min_p)
    nco <- nco[above_min_p, above_min_p]
    
    if ( nodes_to_process > 1 ) {

      cluster_ii_members <- which( nco[1, ] >= min_p)
      
      selected <- nco[cluster_ii_members, cluster_ii_members]
      # assign cluster labels
      community_label <- community_label + 1
      if (is.matrix(selected)) {
        selected_nrow <- nrow(selected)
        for (node_name in colnames(nco)) {
          cons_results$membership[cons_results$name == node_name] <-
            community_label
        }
        nco <- nco[-cluster_ii_members, -cluster_ii_members]
        nodes_to_process <- nodes_to_process - length(cluster_ii_members)
        #print(nodes_to_process)

       } else {
        cons_results$membership[cons_results$name == cluster_ii_members[1]] <- community_label
        nodes_to_process <- 0
      }
    }
  }
  return(cons_results)
}
```

\newpage

```{r}
mu = 35
method <- "IM"

g <- load_benchmark_network(mui = mu, path = path, verbose = TRUE)

#load results of prevous simulations
membership_matrix <- as.matrix(read_csv('n_trials_membership.csv', show_col_types = FALSE))
selected_mui <- (summary_results$mui == mu)
selected_mets <- (summary_results$method == method)
membership_matrix <-   membership_matrix[, (selected_mui & selected_mets)]
# consensus
print("CONSENSUS:")
cons_results <- consensus(membership_matrix, min_p = 0.5)
cons_communities <- make_clusters(g, array(cons_results$membership+1))
cons_communities$algorithm <- paste0(method,"_cons")
cons_communities$rm <-cons_results$rob
results <- analyse_communities(cons_communities, mui, verbose = TRUE)

#add RM to g()
V(g)$rm <- cons_results$rob

table(V(g)$rm)

```
 

 

```{r echo=FALSE}
sc_data <- data.frame(c = V(g)$core, rm = V(g)$rm)
sc_data %>% ggplot(aes(x = c, y = rm))+
    geom_point( color = 'red', size = 3, alpha = .2)+
    labs(title = "coreness and ROBUSTNESS OF MEMBERSHIP of a benchmark network", 
         x = "k-coreness", 
         y = "ROBUSTNESS OF MEMBERSHIP") +
    theme_classic() 
```

\newpage

```{r}
mu = 50
method <- "IM"

g <- load_benchmark_network(mui = mu, path = path, verbose = TRUE)

#load results of prevous simulations
membership_matrix <- as.matrix(read_csv('n_trials_membership.csv',show_col_types = FALSE))
selected_mui <- (summary_results$mui == mu)
selected_mets <- (summary_results$method == method)
membership_matrix <-   membership_matrix[, (selected_mui & selected_mets)]
cons_results <- consensus(membership_matrix, min_p = 0.5)

#make a community object
cons_communities <- make_clusters(g, array(cons_results$membership+1))
cons_communities$algorithm <- paste0(method,"_cons")
cons_communities$rm <-cons_results$rob
results <- analyse_communities(cons_communities, mui, verbose = TRUE)

#add RM to g()
V(g)$rm <- cons_results$rob


```
 

 

```{r echo=FALSE}
sc_data <- data.frame(c = V(g)$core, s = V(g)$rm)
sc_data %>% ggplot(aes(x = c, y = s))+
    geom_point(color = "red", size = 3, alpha = .2)+
    labs(title = "coreness and ROBUSTNESS OF MEMBERSHIP of a benchmark network", x = "k-coreness", y = "ROBUSTNESS OF MEMBERSHIP") +
    theme_classic() 
```

\newpage

# 7- potential improvements by pre-cut

improving performance by pre-cut on methods that underestimate NC

```{r}
pre_cut_network <- function(g,
                            alpha = 0.05,
                            epsilon = 1 / 1000) {
  g_pre_cut <- g
  n_items <- length(E(g_pre_cut))
  n_null <- as.integer(alpha * n_items)
  applied_weights <- E(g_pre_cut)$ww
  applied_weights[sample(n_items, n_null)] <- epsilon
  E(g_pre_cut)$weight <- applied_weights
  return(g_pre_cut)
}
```

example of usage
```{r}
g <- load_benchmark_network(mui = 50, path = path, verbose = TRUE)
gp <- pre_cut_network(g,  alpha = 0.30, epsilon = 1/1000)
hist(strength(g))
hist(strength(gp))
```

\newpage

integrated in cd
```{r final sim , eval=FALSE, include=FALSE}
find_communities_precut <- function(n_trials = 100,
                                     methods ,
                                     mu_values  ,
                                     alpha = 0.6,
                                     epsilon = 0.01,
                                     filename_summary,
                                     filename_membership) {
  results_n_trials <- data.frame()
  membership_matrix <- c()
  for (mu in mu_values) {
    g <- load_benchmark_network(mui = mu, path = path)
    g_precut <- pre_cut_network(g,  alpha = alpha, epsilon = epsilon)

    for (i in 1:n_trials) {
      for (met in methods) {
        tmp_comms <- find_communities(g_precut, method = met)
        results_single <- analyse_communities(tmp_comms, mu)
        results_n_trials = rbind(results_n_trials, results_single)
        membership_matrix <- cbind(membership_matrix, tmp_comms$membership)
      }
    }
  }
  membership_n_trials <- data.frame(membership_matrix)
  membership_n_trials %>% write_csv(filename_membership)
  results_n_trials %>% write_csv(filename_summary)
  return(1)
}

```
 

```{r}
membership_matrix_precut <- find_communities_precut(
  n_trials = 20,
  methods  = c("ML"),
  mu_values = c(20),
  alpha = 0.0,
  epsilon = 0.0001,
  filename_summary = 'n_trials_summary_results_pc.csv',
  filename_membership = 'n_trials_membership_pc.csv'
)

summary_results_pc<- read_csv('n_trials_summary_results_pc.csv')  
membership_matrix_precut <- as.matrix(read_csv('n_trials_membership_pc.csv',show_col_types = FALSE))

print(nrow(membership_matrix_precut))
cons_results_precut <- consensus(membership_matrix_precut, min_p = 0.50)

#make a community object
cons_communities <- make_clusters(g, array(cons_results_precut$membership+1))
cons_communities$algorithm <- paste0(method,"_cons")
cons_communities$rm <-cons_results$rob
results <- analyse_communities(cons_communities, mui, verbose = TRUE)


comms_aggregated <- aggregate_small_communities(cons_communities, min_vids = 5, verbose = TRUE)
results_cons_pruned <- analyse_communities(comms_aggregated, mui, verbose = TRUE)

```


```{r message=FALSE, warning=FALSE, include=FALSE}
mod_to_plot <- summary_results_pc %>% group_by(method, mui) %>% 
    summarise(m = mean(mod), sd = sd(mod) )%>%
  mutate(mu = mui) %>% mutate(mod = m) %>%
  select(method, mu, mod, sd)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
```{r}
mod_to_plot %>%
  ggplot(aes(x = mu, y = mod)) +
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
  geom_rect( aes(NULL, NULL, xmin = 35, xmax = 50, ymin = 0, ymax = 1 ), fill = "lightblue", alpha = 0.01)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
  geom_line(data = subset(mod_to_plot, method != 'built-in'), aes(color = method)) +
  geom_line(data = subset(mod_to_plot, method == 'built-in'), size = 2, color = 'gray') +
  geom_point(data = subset(mod_to_plot, method != 'built-in'), aes(color = method)) +
  geom_point(data = subset(mod_to_plot, method == 'built-in'), size = 1, color = 'black') +
  
     geom_hline(yintercept = 0.5)+
    labs(title = "performance of different methods (modularity)", y = "modularity", x = "mu") +
    theme_light()   

```

# 8- conclusions and future development

verificare che siamo simili al paper single VS consensus) provare
benchmark più simile a quello del paper di riferimento

LP individua tre zone di confidenza

pruning non è pruning ma "reassign labels to community zero"
