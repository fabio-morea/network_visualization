---
title: "Evaluation of the Performance of Community Detection Algorithms of benchmark networks"
author: "Fabio Morea"
date: "2023-05-19"
abstract: "This notebook compares the performance of several community detection methods, and explores the potential improvements that can be obtained by the following techniques: pruning, consensus and pre-cut."
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE)
```

```{r load libraries, include=FALSE}
library(igraph)
library(tidyverse)
library(aricode) #NMI tra vettori
```



# 1- introduction

Network analysis is a powerful tool for understanding complex systems in
both the hard and social sciences. By representing data as a network, it
is possible to identify communities of similar members, or nodes, that
are connected to each other by various relationships. This allows to
gain insights into the structure of the system, as well as the behavior
of the nodes within it.

For example, in social networks, nodes can represent people, and the
relationships between them can represent various types of interactions,
such as friendships or collaborations. By analyzing the network,
researchers can identify clusters of people who are more likely to
interact with each other, or who share similar characteristics.

Our research focuses on the need to identify communities within
networks. This is an unsupervised task that presents several critical
issues, such as selecting the most appropriate algorithm and quantifying
the quality of the results.

The definition of community it depends on the research context and for
our study. We focus on the definition of community as a subset of nodes
that are more strongly connected to each other than to the rest of the
network. A number of Community Detection (CD) methods can be used to
generate partitions of the network by exploiting its topology and
possibly the weight of ties. These methods are widely known and
implemented in programming languages such as R or Python, which enable
the results to be obtained quickly on large networks.

It is essential to recognize that the term 'community detection' can be
misleading. Communities are not an inherent property of the network;
rather, they are the outcome of an algorithm that is dependent on the
characteristics of the network, the parameters chosen, and often an
initial random seed. The results vary depending on the chosen algorithm
and can vary further with each repetition. Since we are in an
unsupervised setting, it would be unwise to take the immediate result
generated by a chosen algorithm at face value. It is wiser to use the
different algorithms available to understand the quality of the result,
and select the algorithm best suited to the situation.

Our research focuses on quantifying the reliability of
community detection results with a 'Node Fuzziness Score' (NFS) applied
to each individual in the network. NFS allows us to gain insights
into the overall quality of the community detection process and to classify
nodes according to their NFS. 

This problem can ge restated as assessing the quality of results produced by CD algorithms over a number of trials, and on a variety of networks with differetns fuzzyness. 

Fuzzyness of the whole network is not measured directly, but a good proxy is to measure Modularity $Q$ of a given partition. The maximum $Q$ obtained by N repetitions of different modularity maximization algorithms can be used as a proxy for fuzzyness. This is consistent with LFR bemchmanrs.

NFS is measured only by pairwise comparison across N independent trials of the same method.
ideally, result obtained by a method should be similar to that
obtained by other methods based on the same community definition.
Moreover, any CD  algorithm should produce the same result at every run (it should not be
stochastic, i.e. it should not depend on random initializations). 

Many algorithms are developed to improve speed, and a results adopt a stochastic approach. As stochasticity is often the byproduct of speed, a good stochastic CD algorithm should quantify the uncertainty associated with its results on the whole network and on the individual node. 

We argue that, before selecting a stochastic CD algorithm for a new network, anexploratory analysis should be performed to asses the overall fuzzyness of the network (i.e. are there any easily identifiable communities within the network?). If the NF Network Fuzzyness is low, an appropriate method can be selected, and results can be confidently interpreted on a single run. In all other cases a more complex approach is required. the researcher must be aware that different methods will generate underestimates or overestimates of the number of communities, assocaited with great variability of results. In this context CD is still possibile by using the following methods
- *aggregation* of small communities (for CD algorithms that produce higly fragmented results)
- *consensus* of independent trials of the same method (to assess NFS)
- *pre-cut* of network (to reduce excessive aggregation of communites)

This notebook addresses all the points mentioned above:
The first part is focused on a family of benchmark networks. As the real-world problems are 
unsupervised, it is impossibile to compare the performance of different methods. 
To study the performance of the method, we will use a family of networks with built-in communities that allow comparison
with true labels. Considerations on the modularity of built-in communities. We will see that the network paramenter \mu is proportional to modularity $Q$ 

The second part is focused on the evaluation of individual methods on single trials and repeated trials.
We observe three parameters: modularity, NMI(true labels) and NC(true
labels). As a function of \mu and \Q . All find excellent solutions for low values of \mu
However, for high \mu they get it wrong and do so in three distinct ways:
overestimation, underestimation and collapse. Even when MU is very high,
we find solutions with higher modularity than the original one. Given that
there is a linear link between MU and modularity, we can find the same
type of response in a Modularity/NMI and Modularity/NC graph. When the
Modularity falls BELOW 0.5, the results decay. The most
interesting is NMI/NC with a target in the origin. We do this on multiple
trials and highlight the three modes:

for MU = 10,20,30 we have "low fuzziness scenario". All methods converge, NMI
high, NC/NCtl = 1. We can also compare NMItrue_labels with
NMI_methods

for MU = 35,40,45 the network has intermediate fuzzyness or "transition" scenario. 
LP has huge variance. some
overestimate and others underestimate

for $\mu<50$ we have "diverging" all methods diverge.

* Network Fuzzyness
* Communlty Label Uncertainty 

## 1.1 summary

The analysis is developed as follows:

1.  Indicators and benchmark networks to assess performance of CD
    methods

2.  community definitions and community detection methods

3.  performance of 6 CD methods on LFR benchmark networks (repeated
    trials). NF Network Fuzzyness scenario

4.  potential improvements by aggregation of small communities

5.  potential improvements by consensus (Node Fuzzybess Score)

6.  potential improvements by pre-cut

7.  conclusions and future development

\newpage

# 2- benchmark networks and Indicators to assess performance of CD methods

## 2.1 benchmark networks

LFR (Lancichinetti-Fortunato-Radicchi) benchmark graphs are synthetic
networks used to evaluate the performance of network analysis
algorithms. They are generated using a stochastic block model and are
designed to have realistic community structure, degree distributions,
and other properties of real-world networks.

They are used to test the accuracy of algorithms for network analysis
tasks such as community detection, link prediction, and node
classification.

Benchmark graphs for testing community detection algorithms, Andrea
Lancichinetti, Santo Fortunato, and Filippo Radicchi, Phys. Rev. E 78,

046110 2008 [LFR_benchmark_graph --- NetworkX 3.1
documentation](https://networkx.org/documentation/stable/reference/generated/networkx.generators.community.LFR_benchmark_graph.html)

The following code chunk defines a function to load a benchmark network
(previously generated).

```{r  echo=TRUE}
load_benchmark_network <- function(mui, path = path, verbose = FALSE) {
    #upload graph
    filename = paste0(path, mui, ".gml")
    g <- read_graph(filename, format = "gml")
    # extract giant component
    #components <- igraph::clusters(g, mode = "weak")
    #g <-induced_subgraph(g, V(g)[components$membership == which.max(components$csize)])
    # set names and weight (ww <- 1.0)
    V(g)$core <- coreness(g)
    V(g)$str <- strength(g)
    V(g)$name <- paste0("V" , V(g)$label)
    E(g)$ww <- 1.0
    # print
    if (verbose == TRUE) {
        print(paste0("Loaded benchmark network ", path, mui, ".gml"))
        print(paste("Giant component size :", length(V(g)$community)))
        print(paste("Built-in communities: ", max(V(g)$community)))
        mod <- round( modularity(g, array(V(g)$community+1) ), digits = 4)
        print(paste("Modularity of built-in communities: ", mod))
    } 
    return(g)
}
```

To illustrate the structure of a reference netowotk, we load a network
with well-defined communities ($mu = 0.20$) and show a coreness-strength
scatterplot.

K-coreness is a measure of the centrality of a node within a network. It
is defined as the largest value of k such that the node belongs to a
subgraph of the network that is composed of nodes with at least degree
k. A node with a high k-coreness is considered to be more central and
influential within the network.

Strength is a measure of the total weight of the connections a node has
to other nodes in the network.

```{r scatter Kcore-Sterngth benchmark, echo=TRUE, message=FALSE, warning=FALSE}
path <- "./LFR_graphs/LFR/LFR_benchmark_"
g <- load_benchmark_network(mui = 20, path = path, verbose = TRUE)
sc_data <- data.frame(c = V(g)$core, s = V(g)$str)
sc_data %>% ggplot(aes(x = c, y = s))+
    geom_point(color = "red", size = 3, alpha = .2)+
    labs(title = "coreness and strength of a benchmark network", x = "k-coreness", y = "strength") +     theme_classic() 
```

```{r scatter nc degree, echo=TRUE, message=FALSE, warning=FALSE}
dg <-  degree(g)
ncd_data <- data.frame(d = dg, nc = as.factor(V(g)$community))
ncd_data %>% ggplot(aes(x = d, y = nc, color = nc))+
  geom_vline(xintercept = mean(dg),  color = 'black', linewidth = 1, alpha = 1)+
  geom_point( size = 3, alpha = .2)+
  labs(title = "built-in communities", x = "degree", y = "community ID") +     theme_classic() 
```

The family of benchmark networks can be characterized by the modularity
of its built-in communities.

```{r}
mod_benchmarks <- data.frame(method = character(), mu = numeric(), modularity = numeric())
method <- "built-in"
for (mu in seq(10, 70, 5)) {
    g <- load_benchmark_network(mui = mu, path = path)
    mod <- modularity(g, make_clusters(g,V(g)$community+1 )$membership )
    mod_benchmarks <- rbind(mod_benchmarks, data.frame(method,mu, mod))
}
mod_benchmarks %>% 
  ggplot(aes(x = mu, y = mod)) +
  geom_line(linewidth = 2, color = 'gray')+
  geom_point()+
  ylim(0,1) + theme_classic()
```

Methods based on modulartiy maximization are less effective with high
values of \mu.

\newpage

## 2.2 indicators

Modularity $Q$ and Max(Q)that is a proxy of network fuzzyness

NMI 
NC number of communities NMI can be useful to measure the agreement
of two independent label assignments on the same dataset when the real
ground truth is not known.

\newpage

# 3- community definitions and community detection methods

The definition of 'community' in network analysis depends on the context
and the researcher's goals. for example:

1.  a community is a group of nodes that are more densely connected to
    each other than to the rest of the network.

2.  a community is group of nodes that share certain characteristics,
    such as having similar properties

3.  a community is a group of nodes that are more likely to interact
    with each other than with other nodes in the network.

In this paper we will focus only on the first definition, and we will
test several metohds that are consisten with that definition. Namely

-   FG Fast Greedy

-   IM infomap

-   LP Label Propagation

-   ML Multilevel (aka Louvain)

-   WT walktrap

```{r function_find_communities}
find_communities <- function(g, method, verbose = FALSE) {
    if (method == "LV"){
        comms <- cluster_louvain(g, resolution = 1.0)
    } else if (method == "FG"){
        comms <- fastgreedy.community(g)
    } else if (method == "IM"){
        comms <- infomap.community(g)
    } else if (method == "LP"){
        comms <- label.propagation.community(g)
    } else if (method == "ML"){
        comms <- multilevel.community(g)
    } else if (method == "WT"){
        comms <- walktrap.community(g)
    } else {
        print("No valid method")
        stop
    }
    comms$algorithm = method
    if (verbose == TRUE) {print(paste("Community detection with ", method, "completed."))}
    return(comms)
}
```

Each method will be explored separately, in a single trial on a LFR
benchmark network, highlighting the main results with the followin
gfunction

```{r function_analyse_communities}
analyse_communities <- function(communities, mui, verbose = FALSE){
    method <- communities$algorithm
    c_membership <- communities$membership
    mod <- round(modularity (g,  c_membership+1), digits = 4)
    # need to use c_membership + 1 to handle community label 0
    nc <- length(table(c_membership))
    # NMI against "Built In Communities"
    nmi = round(aricode::NMI( as.factor(V(g)$community),as.factor(c_membership)), 3)
    if(verbose == TRUE){
        print(paste("Communities found: ",nc))
        print(paste("Modularity: ", mod))
        print(paste("Normalized Mutual Information between C.D. method and built-in communities:", nmi))
    } 
    return(data.frame(mui, method , mod, nc, nmi))
}
```

all trials use the same network

```{r}
mui = 30
g <- load_benchmark_network(mui = mui, path = path, verbose = TRUE)
```

\newpage

## 3.1 Method FG Fast Greedy

This algorithm was proposed by Clauset et al.12. It is a greedy
community analysis algorithm that optimises the modularity score. This
method starts with a totally non-clustered initial assignment, where
each node forms a singleton community, and then computes the expected
improvement of modularity for each pair of communities, chooses a
community pair that gives the maximum improvement of modularity and
merges them into a new community. The above procedure is repeated until
no community pairs merge leads to an increase in modularity. For sparse,
hierarchical, networks the algorithm runs in $O$(N log2(N)).


```{r}
g <- load_benchmark_network(mui = mui, path = path)
tmp_comms <- find_communities(g, method = "FG", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

## 3.2 Method IM infomap

This algorithm was proposed by Rosvall et al.35,36. It figures out
communities by employing random walks to analyse the information flow
through a network17. This algorithm starts with encoding the network
into modules in a way that maximises the amount of information about the
original network. Then it sends the signal to a decoder through a
channel with limited capacity. The decoder tries to decode the message
and to construct a set of possible candidates for the original graph.
The smaller the number of candidates, the more information about the
original network has been transferred. This algorithm runs in $O$(E).

```{r}
tmp_comms <- find_communities(g, method = "IM", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

## 3.3 method LP Label Propagation

This algorithm was introduced by Raghavan et al.38. It assumes that each
node in the network is assigned to the same community as the majority of
its neighbours. This algorithm starts with initialising a distinct label
(community) for each node in the network. Then, the nodes in the network
are listed in a random sequential order. Afterwards, through the
sequence, each node takes the label of the majority of its neighbours.
The above step will stop once each node has the same label as the
majority of its neighbours. The computational complexity of label
propagation algorithm is $O$(E).

```{r}
g <- load_benchmark_network(mui = mui, path = path)
tmp_comms <- find_communities(g, method = "IM", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

## 3.4 method ML Multilevel (Louvain)

This algorithm was introduced by Blondel et al.. It is a different
greedy approach for optimising the modularity with respect to the
Fastgreedy method. This method first assigns a different community to
each node of the network, then a node is moved to the community of one
of its neighbours with which it achieves the highest positive
contribution to modularity. The above step is repeated for all nodes
until no further improvement can be achieved. Then each community is
considered as a single node on its own and the second step is repeated
until there is only a single node left or when the modularity can't be
increased in a single step. The computational complexity of the
Multilevel algorithm is $O$(N log N). Blondel, V. D., Guillaume, J.-L.,
Lambiotte, R. & Lefebvre, E. Fast unfolding of communities in large
networks. Journal of Statistical Mechanics: Theory and Experiment 2008,
P10008 (2008).

```{r}
g <- load_benchmark_network(mui = mui, path = path)
tmp_comms <- find_communities(g, method = "ML", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

## 3.5 method WT walktrap

This algorithm was proposed by Pon & Latapy. It is a hierarchical
clustering algorithm. The basic idea of this method is that short
distance random walks tend to stay in the same community. Starting from
a totally non-clustered partition, the distances between all adjacent
nodes are computed. Then, two adjacent communities are chosen, they are
merged into a new one and the distances between communities are updated.
This step is repeated (N-1) times, thus the computational complexity of
this algorithm is $O$(E N2). For sparse networks the computational
complexity is $O$(N2 log(N)

```{r}
g <- load_benchmark_network(mui = mui, path = path)
tmp_comms <- find_communities(g, method = "WT", verbose = FALSE)
results <- analyse_communities(tmp_comms, mui, verbose = TRUE)
```

\newpage

# 4- performance of 6 CD methods on LFR benchmark networks (repeated trials)

Exploring the variations of results: we generate 200 trials for a range
of MU and save results in a csv file.

```{r MAIN SIMULATION, eval=FALSE, include=FALSE}
n_trials = 10
results_n_trials <- data.frame()
membership_matrix <- c()
all_methods <- c("FG", "IM", "LP", "ML", "WT")
g <- load_benchmark_network(mui = mui, path = path)
for (mui in seq(10, 70, 5)) {
  g <- load_benchmark_network(mui = mui, path = path)
  for (i in 1:n_trials) {
    for (met in all_methods) {
      tmp_comms <- find_communities(g, method = met)
      results_single <- analyse_communities(tmp_comms, mui)
      results_n_trials = rbind(results_n_trials, results_single)
      membership_matrix <- cbind(membership_matrix, tmp_comms$membership)
    }
  }
}
membership_n_trials <- data.frame(membership_matrix) 
membership_n_trials %>% write_csv('n_trials_membership.csv')
results_n_trials %>% write_csv('n_trials_summary_results.csv')
```
 


```{r read csv repeated results, message=FALSE, warning=FALSE}
summary_results<- read_csv('n_trials_summary_results.csv')  
#membership_matrix <- as.matrix(read_csv('n_trials_membership.csv'))
```

## 4.1 performance of different methods over repeated trials: modularity

```{r message=FALSE, warning=FALSE, include=FALSE}
mod_to_plot <- summary_results %>% group_by(method, mui) %>% 
    summarise(m = mean(mod), sd = sd(mod) )%>%
  mutate(mu = mui) %>% mutate(mod = m) %>%
  select(method, mu, mod, sd) %>% 
  rbind(mod_benchmarks) %>% 
  mutate(nm = mod/mod_benchmarks$mod) 
```

```{r}
mod_to_plot %>%
  ggplot(aes(x = mu, y = mod)) +
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
  geom_rect( aes(NULL, NULL, xmin = 35, xmax = 50, ymin = 0, ymax = 1 ), fill = "lightblue", alpha = 0.01)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
  geom_line(data = subset(mod_to_plot, method != 'built-in'), aes(color = method)) +
  geom_line(data = subset(mod_to_plot, method == 'built-in'), size = 2, color = 'gray') +
  geom_point(data = subset(mod_to_plot, method != 'built-in'), aes(color = method)) +
  geom_point(data = subset(mod_to_plot, method == 'built-in'), size = 1, color = 'black') +
  
     geom_hline(yintercept = 0.5)+
    labs(title = "performance of different methods (modularity)", y = "modularity", x = "mu") +
    theme_light()   

```

Note that when modularity \< 0.5, the definition of community is not
consistent.
WHen LP returns modularity > 0.5 it is very accurate. returns modularity < 0.5, the algorithm fails. This is a very relevant insight
try modularity / modularityBI

```{r}
mod_to_plot %>% filter(method != 'built-in') %>%
  ggplot(aes(x = mu, y = nm)) +
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 2 ), fill = "green", alpha = 0.005)+
  geom_rect( aes(NULL, NULL, xmin = 35, xmax = 50, ymin = 0, ymax = 2 ), fill = "lightblue", alpha = 0.01)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 2 ), fill = "yellow", alpha = 0.005)+

  geom_line( aes(color = method)) +
  geom_point( aes(color = method)) + 
    labs(title = "different performance of CD methods as a function of network fuzzynes", y = "modularity method / modularity built-in", x = "mu") +
    theme_minimal()   

```

## 4.2 performance of different methods over repeated trials: NMI

NMI can be calculated against the built-in communities (NMI-BI) or
against all other methods (NMI_OM).

```{r}
summary_results %>% 
    filter(method %in% c( "FG", "IM", "LP", "ML", "WT")) %>%
  ggplot(aes(x = mui, y = nmi)) +
    geom_point(aes(color = method)) +
    labs(title = "Performance of different methods over N trials", y = "NMI_BIC", x = "mu") +
    theme_light()  + 
    facet_wrap(method ~ .) #,  scales = "free_x"
```

```{r}
summary_results %>% 
  filter(mui %in% c(40)) %>%
  filter(method %in% c( "FG", "IM", "LP", "ML", "WT")) %>%
    ggplot(aes(x = nmi)) +
    geom_histogram(color = "black", fill = "red", bins = 100) +
    labs(title = "Performance of different methods over N trials mu = 40", x = "Normalized Mutual Information", y = "count") +
    geom_vline(xintercept = 0.5, color = 'red')+
    theme_classic() + 
    facet_wrap(method ~ .)
```

\newpage

The following Figure compares the results of each methods with the built
in labels, using NMI.

```{r}
summary_results %>% group_by(method, mui) %>% 
    summarise(mean_nmi = mean(nmi), sd_nmi = sd(nmi) ) %>%
    ggplot(aes(x = mui, y = mean_nmi)) +
    geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
    geom_rect( aes(NULL, NULL, xmin = 35, xmax = 50, ymin = 0, ymax = 1 ), fill = "lightblue", alpha = 0.01)+
    geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
    geom_line(aes(color = method)) +
    geom_point(aes(color = method)) +
    geom_linerange(aes(ymin =mean_nmi-sd_nmi, ymax = mean_nmi + sd_nmi, color = method), linewidth = 2, alpha = 0.5)+
    geom_hline(yintercept = 0.5)+
    labs(title = "performance of different methods ", y = "NMI_BIC", x = "mu") +
    theme_light()  
```

The following Figure compares the results of each methods with the built
in labels, using NMI.

```{r}
summary_results %>% group_by(method, mui) %>% 
    summarise(mean_nmi = mean(nmi), sd_nmi = sd(nmi), 
              mean_mod = mean(mod), sd_mod = sd(mod) ) %>%
    ggplot(aes(x = mean_mod, y = mean_nmi)) +
    geom_rect( aes(NULL, NULL, xmin = 0.6 , xmax = 1.0, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
    geom_rect( aes(NULL, NULL, xmin = 0.35 , xmax = 0.6, ymin = 0, ymax = 1 ), fill = "lightblue", alpha = 0.01)+
    geom_rect( aes(NULL, NULL, xmin = 0.0 , xmax = 0.35, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+

      geom_line(aes(color = method)) +
    geom_point(aes(color = method)) +
    #geom_linerange(aes(ymin = mean_nmi - sd_nmi, ymax = mean_nmi + sd_nmi, color = method), linewidth = 2, alpha = 0.5)+
    #geom_linerange(aes(xmin = mean_mod - sd_mod, xmax = mean_mod + sd_mod, color = method), linewidth = 2, alpha = 0.5)+
      geom_hline(yintercept = 0.5)+
    labs(title = "performance of different methods ", y = "NMI_BIC", x = "modularity") +
    theme_minimal()  
```

```{r}
 data_points <- summary_results
data_lines <- summary_results %>% group_by(method, mui) %>% 
    summarise(mean_nmi = mean(nmi), sd_nmi = sd(nmi), 
              mean_mod = mean(mod), sd_mod = sd(mod) ) 
ggplot() +
    geom_line(data = data_lines, aes(x = mean_mod, y = mean_nmi, color = method)) +
    geom_linerange(data = data_lines, aes(x = mean_mod, y = mean_nmi,ymin = mean_nmi - sd_nmi, ymax = mean_nmi + sd_nmi, color = method), linewidth = 2, alpha = 0.5)+
    geom_linerange(data = data_lines, aes(x = mean_mod, y = mean_nmi,xmin = mean_mod - sd_mod, xmax = mean_mod + sd_mod, color = method), linewidth = 2, alpha = 0.5)+
      geom_point(data = data_points, aes(x = mod, y = nmi, color = method)) +

      geom_hline(yintercept = 0.5)+
    labs(title = "performance of different methods ", y = "NMI_BIC", x = "modularity") +
    theme_light()  + facet_wrap(method ~ .)
```

\newpage

## community detection to assess "fuzzyness"

When dealing with an unsupervised community detection task, it is
important to remember that the term "detection" is misleading.
Communities are not an intrinsic property of the network, but rather
arise as the result of an algorithm. The results depend on network
properties, the chosen algorithm, its parameters, and its intrinsic
stochasticity.

Additionally, networks may generate communities with different levels of
fuzziness. Low fuzziness refers to well-defined communities in which
members have strong connections with each other and weak connections to
members of other communities. High fuzziness, on the other hand, refers
to communities in which members have weaker connections with each other
and stronger connections to members of other communities. In between
these two extremes, there is a transition zone, where the strongly
fuzziness depends on the method used to measure it.

Low fuzzyness: modularity is HIGH \> 0.5 is always high regardless of
the method. LP and FG are sensitive to fuzzyness: when their results
drop below 0.5, we are in the transition TRY NMI without true labels

```{r}
summary_results %>% 
  filter(mui %in% c(10,20,30)) %>% 
  ggplot(aes(x = mod, y = nmi, color = method)) +
  geom_point( size = 4, alpha = 0.2) +
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 0.5, color = "red")+
    labs(title = "performance of different methods LOW FUZZYNESS", y = "NMI_BIC", x = "modularity") +
  theme_light()  + facet_wrap(mui ~ .)
```

```{r}
summary_results %>% 
  filter(mui %in% c(35,40)) %>% 
  ggplot(aes(x = mod, y = nmi, color = method)) +
  geom_point( size = 4, alpha = 0.5) +
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 0.5, color = "red")+
  labs(title = "performance of different methods TRANSITION ", y = "NMI_BIC", x = "modularity") +
  theme_light()  + facet_wrap(mui ~ .)
```

when most methods generate low NMI, we

```{r}
summary_results %>% 
  filter(mui %in% c(50,60)) %>% 
  ggplot(aes(x = mod, y = nmi, color = method)) +
  geom_point(size = 4, alpha = 0.2 ) +
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 0.5, color = "red")+
  labs(title = "performance of different methods HIGH FUZZYNESS ", y = "NMI_BIC", x = "modularity") +
  theme_light()  + facet_wrap(mui ~ .)
```

\newpage

```{r}
summary_results %>% 
  filter(mui %in% c(20,40,60)) %>% 
  group_by(method, mui) %>% 
    summarise(mean_nmi = mean(nmi), sd_nmi = sd(nmi), 
              mean_mod = mean(mod), sd_mod = sd(mod) ) %>%
  mutate( fuzzyness = case_when(
    mui == 20 ~ '  low fuzzyness',
    mui == 40 ~ ' transition',
    mui == 60 ~ 'high fuzzyness'))%>%
  ggplot(aes(x = mean_mod, y = mean_nmi, shape = as.factor(method), color = fuzzyness)) +
  geom_point(size = 3, alpha = 1 ) +
  geom_linerange(aes(x = mean_mod, y = mean_nmi,ymin = mean_nmi - sd_nmi, ymax = mean_nmi + sd_nmi), 
                 linewidth = 2, alpha = 0.5)+
geom_linerange(aes(x = mean_mod, y = mean_nmi,xmin = mean_mod - sd_mod, xmax = mean_mod + sd_mod), linewidth = 2, alpha = 0.5)+
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 0.5, color = "red")+
  xlim(0,1)+#ylim(0,1)+
  labs(title = "Fuzzyness of communities", x = "modularity") +
  theme_light()  #+ facet_wrap(fuzzyness ~ .)
```

-   Low fuzzyness: all methods converge to the same result. Modularity
    is above 0.5
-   High fuzzyness: LP collapses to a single community, all other
    methods generate m \< 0.5
-   Transition:Other methods diverge: IM, ML and WT are above; FG drops
    below 0.5; LP generates high variance, including collapse.

if Low fuzzyness: use LP results if Transition: use IM, ML or WT, and
aggregate small communities if High fuzzyness: use IM or WT, precut,
consensus and aggregate small communities \newpage

## 4.3 performance of different methods over repeated trials: Number of Communities

```{r}
summary_results %>% 
    filter(method %in% c( "FG", "ML")) %>%
  ggplot(aes(x = mui, y = nc)) +
    geom_point(aes(color = method)) +
    labs(title = "Performance of different methods over N trials", y = "Number of Communitiess", x = "mu") +
  geom_hline(yintercept = 37)+
    theme_light() + 
    facet_wrap(method ~ . )
```

\``{r} summary_results %>%      filter(method %in% c( "LP")) %>%   ggplot(aes(x = mui, y = nc)) +     geom_point(aes(color = method)) +     labs(title = "Performance of different methods over N trials", y = "Number of Communitiess", x = "mu") +   geom_hline(yintercept = 37)+     theme_light() +      facet_wrap(method ~ . )`

```{r}
summary_results %>% 
    filter(method %in% c( "IM",  "WT")) %>%
  ggplot(aes(x = mui, y = nc)) +
    geom_point(aes(color = method)) +
    labs(title = "Performance of different methods over N trials", y = "Number of Communities", x = "mu") +
  geom_hline(yintercept = 37)+
    theme_light() + 
    facet_wrap(method ~ . , scales = 'free_y')
```

```{r}

summary_results %>% 
    group_by(method, mui) %>% 
    summarise(mean_nc = mean(nc), sd_nc = sd(nc)) %>%
    ggplot(aes(x = mui, y = mean_nc)) +
    geom_point(aes(color = method)) +
    geom_line(aes(color = method)) +
    geom_linerange(aes(ymin =mean_nc-sd_nc, ymax = mean_nc + sd_nc, color = method), linewidth = 2, alpha = 0.5)+
    geom_hline(yintercept = 37, color = 'red')+

    labs(title = "performance of different methods (Number of Communities)", y = "number of communities", x = "mu") +
    theme_light()    
```

\newpage

```{r}
summary_results %>% 
  filter(mui %in% c(20,40,60)) %>% 
  group_by(method, mui) %>% 
    summarise(mean_nc = mean(nc)/37, sd_nc = sd(nc)/37, 
              mean_mod = mean(mod)  ) %>%
  mutate( fuzzyness = case_when(
    mui == 20 ~ '  low fuzzyness',
    mui == 40 ~ ' transition',
    mui == 60 ~ 'high fuzzyness'))%>%
  ggplot(aes(x = mean_mod, y = mean_nc, shape = as.factor(method), color = fuzzyness)) +
  geom_point(size = 3, alpha = 1 ) +
  geom_linerange(aes(x = mean_mod, y = mean_nc,ymin = mean_nc - sd_nc, ymax = mean_nc + sd_nc), 
                 linewidth = 2, alpha = 0.5)+
  geom_vline(xintercept = 0.5, color = "red")+
  geom_hline(yintercept = 1, color = "red")+
  xlim(0,1)+#ylim(0,1)+
  labs(title = "Fuzzyness of communities", x = "modularity") +
  theme_light() # + facet_wrap(fuzzyness ~ .)
```

How to improve results if fuzzyness low: use LP or WT if fuzzyness high:
use WT, IM with consensus collapse. or use ML, FG with precut consensus
and collapse if transition: use WT, IM with collapse \newpage

## 4.4 overal performance NMI NC

Performance measured as NMI and NC (improvement: normalize NC/37)

```{r}
summary_results %>% filter(mui %in%c(20,40,60)) %>%
    ggplot(aes(x = nmi, y = nc/37)) +
    geom_point(aes(color = method))+
    labs(title = "performance of different methods (NMI VS Number of Communities normalized)", 
         y = "number of communities", x = "NMI") +
    geom_hline(yintercept = 1.0, color = 'red')+
    geom_vline(xintercept = 1.0, color = 'red')+
    theme_light()  + facet_wrap(mui ~ .)
```

\newpage

## comparing NMI with other methods (without Built-in communities)

```{r compare all NMI}
membership_matrix <- as.matrix(read_csv('n_trials_membership.csv'))
nmis <- data.frame()
for (mui in seq(10, 70, 5)) {
  membership_matrix_mu <- membership_matrix[, summary_results$mui == mui]
  methods <- summary_results$method[summary_results$mui == mui]
  mts <- c("FG", "IM", "LP", "ML", "WT")
  for (met1 in 1:length(mts)) {
    for (met2 in met1:length(mts)) {
      if (met1 != met2) {
        mm1 <- mts[met1]
        mm2 <- mts[met2]
        mb1 <- membership_matrix_mu[, methods == mm1]
        mb2 <- membership_matrix_mu[, methods == mm2]
        #print(paste(mui,"calculating NMI between methods", met1, met2))
        for (i in 1:ncol(mb1)) {
          for (j in i:ncol(mb2)) {
            if (i != j) {
              nmi_m1_m2 <- aricode::NMI(mb1[, i], mb2[, j])
              nmis <-rbind(nmis,data.frame(mui, mm1,mm2, met1, met2, nmi_m1_m2))
            }
          }
        }
      }
    }
  }
}
nmis %>% write_csv('NMIs.csv')
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
nmis <- read_csv('NMIs.csv')
NMI <- nmis %>% 
  group_by(mui, mm1, mm2)%>%
  summarize(nmi = mean(nmi_m1_m2), sd_nmi =sd(nmi_m1_m2))%>%
  mutate(methods = str_c(mm1, "-", mm2))

NMI%>%
  ggplot()+
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
  geom_rect( aes(NULL, NULL, xmin = 35, xmax = 50, ymin = 0, ymax = 1 ), fill = "lightblue", alpha = 0.01)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
  geom_line(aes(x = mui, y = nmi, color = methods)) + 
  #geom_linerange(aes(x = mui, y = nmi ,ymin = nmi - sd_nmi, ymax = nmi + sd_nmi), linewidth = 2, alpha = 0.5)+
  theme_light()+ggtitle("NMI - Pairwise comparison ")


```

...
...
...

```{r echo=TRUE, message=FALSE, warning=FALSE}
nmis <- read_csv('NMIs.csv')
NMI <- nmis %>% 
  group_by(mui, mm1, mm2)%>%
  summarize(nmi = mean(nmi_m1_m2), sd_nmi =sd(nmi_m1_m2))%>%
  mutate(methods = str_c(mm1, "-", mm2))

NMI%>% filter(methods %in% c( "IM-LP", "LP-ML", "LP-WT"))%>%
  ggplot()+
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
  geom_rect( aes(NULL, NULL, xmin = 35, xmax = 50, ymin = 0, ymax = 1 ), fill = "lightblue", alpha = 0.01)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
  geom_line( aes(x = mui, y = nmi, color = methods)) + 
  #geom_linerange(aes(x = mui, y = nmi ,ymin = nmi - sd_nmi, ymax = nmi + sd_nmi), linewidth = 2, alpha = 0.5)+
  theme_light()+ggtitle("NMI - Pairwise comparison with LP")


```


```{r}
NMI%>% filter(methods %in% c("IM-WT", "FG-ML", "IM-FG"))%>%
  ggplot()+
  geom_rect( aes(NULL, NULL, xmin = 10 , xmax = 35, ymin = 0, ymax = 1 ), fill = "green", alpha = 0.005)+
  geom_rect( aes(NULL, NULL, xmin = 35, xmax = 50, ymin = 0, ymax = 1 ), fill = "lightblue", alpha = 0.01)+
  geom_rect( aes(NULL, NULL, xmin = 50, xmax = 70, ymin = 0, ymax = 1 ), fill = "yellow", alpha = 0.005)+
  geom_line( aes(x = mui, y = nmi, color = methods)) + 
  #geom_linerange(aes(x = mui, y = nmi ,ymin = nmi - sd_nmi, ymax = nmi + sd_nmi), linewidth = 2, alpha = 0.5)+
  theme_light()+ggtitle("NMI - Pairwise comparison - extremes ")
```



```{r}
nmis %>% filter(mui %in% c(10,30, 50))%>%
  ggplot()+
  geom_point(aes(x=mm1, y = mm2, size = nmi_m1_m2^2, color = nmi_m1_m2))  + 
  scale_colour_gradientn(colours=rainbow(2))+
  facet_wrap(mui ~ .) + theme_light() + 
  ggtitle("Normalized Mutual Information between results obtained by different CD methods")
```

```{r}
edgelist <-nmis %>% mutate(weight = nmi_m1_m2) %>% select(-met1,-met2,-nmi_m1_m2)
ggg <- graph_from_data_frame(edgelist)
ggg <- as.undirected(ggg,mode = "collapse", edge.attr.comb = "sum")
plot(ggg, edge.width=E(ggg)$weight*0.5)
```

alternative: projection on 1-dimension alternative: take one as a
reference and compare with the others

\newpage

# 5- potential improvements by aggregation of small communities

re-assigning small communities to "community 999" can improve NMI and NC
of course only for the methods that overestimate NC

```{r pruning definition}
aggregate_small_communities <- function(comms, min_vids, verbose = FALSE) {
    comms_membership <- comms$membership 
    cs <- table(comms_membership)
    for (i in 1:max(comms_membership)) {
        if (cs[i] < min_vids) {
            comms_membership[comms_membership == i] <- 0
        }
    }
    comms$membership <- comms_membership
    if (verbose == TRUE) { 
        print(paste("Pruning communities below ", min_vids))}
    return(comms)
}

```

Example of pruning

```{r pruning example1}
mui = 30
method = "IM"
g <- load_benchmark_network(mui = mui, path = path, verbose = TRUE)
comms_sample <- find_communities(g, method = method, verbose = TRUE)
results <- analyse_communities(comms_sample, mui, verbose = TRUE)

comms_aggregated <- aggregate_small_communities(comms_sample, min_vids = 10, verbose = TRUE)
results_aggregated <- analyse_communities(comms_aggregated, mui, verbose = TRUE)
```

The number of communities is closer to the built-in value (37), while
there is no relevant change in NMI.

The following shows how pruning impacts NC, usimg method IM and
$\mu = 0.30$

```{r pruning example3}
mui = 30
method = "IM"
g <- load_benchmark_network(mui = mui, path = path, verbose = TRUE)
comms_sample <- find_communities(g, method = method, verbose = TRUE)
results <- analyse_communities(comms_sample, mui, verbose = TRUE)
for (min_vids in 1:12){
    comms_aggregated <- aggregate_small_communities(comms_sample, min_vids = min_vids, verbose = FALSE)
    results_aggregated <- analyse_communities(comms_aggregated, mui , verbose = FALSE)
    print(paste0("min_vids = ", min_vids, "  NMI = ", results_aggregated$nmi,"  NC = ",results_aggregated$nc ))
}

```

\newpage

# 6- potential improvements by consensus

Consensus helps improving performance (applicable to methods that
underestimate NV, as it generates a number of small communities, AND
have some intrinsic variability of results)

```{r}
normalized_co_occurrence <- function(all_clusters) {
  n_trials <- ncol(all_clusters)
  x <-matrix(0,
           nrow = nrow(all_clusters),
           ncol = nrow(all_clusters))
  colnames(x) <- V(g)$name
  rownames(x) <- V(g)$name
  for (i in (1:n_trials)) {
    nclusters <- max(all_clusters[, i])
    for (k in 1:nclusters) {
      samecluster <- (which(all_clusters[, i] == k))
      nc <- length(samecluster)
      for (t in 1:nc) {
        for (j in 1:nc) {
          x[samecluster[j], samecluster[t]] <-
            x[samecluster[j], samecluster[t]] + 1
        }
      }
    }
  }
  x <- x / ncol(all_clusters)
  diag(x) <- 0
  return (x)
}
```

\newpage

```{r}
consensus <-function(all_clusters, min_p = 0.01,min_vids = 1,verbose = FALSE) {
    remaining <-normalized_co_occurrence(all_clusters)
    v.processed <- 0
    current.cluster = 0
    ccs <- data.frame(name = V(g)$name)
    ccs$mbshp = rep(0, nrow(ccs))
    ccs$prob = apply(remaining, 1, max)
    more_clusters_to_process = TRUE
    while (more_clusters_to_process) {
      cluster_ii_members <- which(remaining[1, ] > min_p)
      v.processed <- v.processed + length(cluster_ii_members)
      if (verbose == TRUE) {
        print(paste("start While loop with v to process = ",dim(remaining)[1],v.processed))
      }
      selected <-remaining[cluster_ii_members, cluster_ii_members]
      if (is.matrix(selected)) {
        #diag(selected) <- 0 # diagonal elements are not relevant
        if ( (length(cluster_ii_members) > min_vids) == TRUE) {
          current.cluster <- current.cluster + 1
          if (verbose == TRUE) {
            print(paste("community",current.cluster,max(selected),length(cluster_ii_members)))
          }
          for (j in 1:nrow(selected)) {
            nn <- names(selected[1, ])[j]
            if (verbose == TRUE)  {
              print(paste("Adding",nn,"to comm",current.cluster))
            }
            ccs$mbshp[ccs$name == nn] <- current.cluster
            ccs$prob[ccs$name == nn] <- max(selected[j, ])
          }
        } else {
          if (verbose == TRUE)  {
            print(paste("community zero",max(selected),length(cluster_ii_members)))
          }
          for (j in 1:nrow(selected)) {
            nn <- names(selected[1, ])[j]
            ccs$mbshp[ccs$name == nn] <-  0
            ccs$prob[ccs$name == nn] <-  max(selected)
          }
        }
      }
      tmp <-remaining[-cluster_ii_members,-cluster_ii_members]
      if (is.matrix(tmp)) {
        if (dim(tmp)[1] <= 1) {
          more_clusters_to_process <- FALSE
        }
        remaining <- tmp
      } else {
        more_clusters_to_process <- FALSE
      }
    }
    return(ccs)
  }
```

\newpage

Examples

```{r create all_clusters and CONSENSUS }
n_trials <- 100
method <- 'ML'
all_clusters <- c()
mui = 20
g <- load_benchmark_network(mui = mui, path = path, verbose = TRUE)
for (i in 1:n_trials){
    comms_i <- find_communities(g, method = method)
    all_clusters <- cbind(all_clusters, comms_i$membership)
}
#print("Results of last single trial")
results <- analyse_communities(comms_i, mui, verbose = TRUE)

#print("Results of consensus")
cons_results <- consensus(all_clusters, min_p = 0.5)
cons_communities <- make_clusters(g, array(cons_results$mbshp+1))
cons_communities$algorithm <- paste0(method,"_cons")
cons_communities$rm <-cons_results$prob
V(g)$rm <- cons_results$prob

results_cons <- analyse_communities(cons_communities, mui, verbose = TRUE)

```

```{r echo=FALSE}
sc_data <- data.frame(c = V(g)$core, s = V(g)$rm)
sc_data %>% ggplot(aes(x = c, y = s))+
    geom_point(color = "red", size = 3, alpha = .2)+
    labs(title = "coreness and ROBUSTNESS OF MEMBERSHIP of a benchmark network", x = "k-coreness", y = "ROBUSTNESS OF MEMBERSHIP") +
    theme_classic() 
```

```{r}
hist(cons_results$prob)
```

\newpage

# 7- potential improvements by pre-cut

improving performance by pre-cut on methods that underestimate NC

```{r}
pre_cut_network <- function(g,
                            alpha = 0.05,
                            epsilon = 1 / 1000) {
  g_pre_cut <- g
  n_items <- length(E(g_pre_cut))
  n_null <- as.integer(alpha * n_items)
  applied_weights <- E(g_pre_cut)$ww
  applied_weights[sample(n_items, n_null)] <- epsilon
  E(g_pre_cut)$weight <- applied_weights
  return(g_pre_cut)
}
```

```{r}
g <- load_benchmark_network(mui = 50, path = path, verbose = TRUE)
gp <- pre_cut_network(g,  alpha = 0.30, epsilon = 1/1000)
hist(strength(g))
hist(strength(gp))
```

\newpage

```{r   }
n_trials <- 100
method <- 'LV'
all_clusters <- c()
mui = 30
g <- load_benchmark_network(mui = mui, path = path, verbose = TRUE)

for (i in 1:n_trials){
  gp <- pre_cut_network(g,  alpha = 0.10, epsilon = 1/1000)
  comms_i <- find_communities(gp, method = method)
  all_clusters <- cbind(all_clusters, comms_i$membership)
}
#print("Results of last single trial")
results <- analyse_communities(comms_i, mui, verbose = TRUE)

#print("Results of consensus")
cons_results <- consensus(all_clusters, min_p = 0.5)
cons_communities <- make_clusters(gp, array(cons_results$mbshp+1))
cons_communities$algorithm <- paste0(method,"_cons")
cons_communities$rm <-cons_results$prob

results_cons <- analyse_communities(cons_communities, mui, verbose = TRUE)
V(gp)$rm <- cons_results$prob

comms_aggregated <- aggregate_small_communities(comms_sample, min_vids = 10, verbose = TRUE)
results_cons_pruned <- analyse_communities(comms_aggregated, mui, verbose = TRUE)
```

-------- replicare qui i risultati NMI e NC in funzione di MU

```{r}

```

# 8- conclusions and future development

verificare che siamo simili al paper single VS consensus) provare
benchmark più simile a quello del paper di riferimento

LP individua tre zone di confidenza

pruning non è pruning ma "reassign labels to community zero"
